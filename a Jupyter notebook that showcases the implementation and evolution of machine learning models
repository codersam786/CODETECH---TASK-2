# In this notebook, we will showcase the evolution of machine learning models on a classification task.
# We will start with a simple Logistic Regression model and progress towards more complex models such as Random Forest and Neural Networks.
# The dataset used in this example is the popular Iris dataset.

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preview of the dataset
print("Dataset Features: ", data.feature_names)
print("Dataset Target Classes: ", data.target_names)

# Stage 1: Implementing Logistic Regression
from sklearn.linear_model import LogisticRegression

# Initialize the model
log_reg = LogisticRegression(max_iter=200)

# Train the model
log_reg.fit(X_train, y_train)

# Predictions
y_pred_log_reg = log_reg.predict(X_test)

# Evaluate the model
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
print(f"Logistic Regression Accuracy: {accuracy_log_reg:.4f}")

# Visualization of decision boundaries (optional for 2D data, not used here due to Iris dataset having 4 features)


# Stage 2: Implementing Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

# Initialize the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")

# Feature importances visualization
features = data.feature_names
importances = rf_model.feature_importances_

plt.barh(features, importances)
plt.title("Random Forest Feature Importance")
plt.show()


# Stage 3: Implementing a Neural Network (MLP)
from sklearn.neural_network import MLPClassifier

# Initialize the model
mlp_model = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)

# Train the model
mlp_model.fit(X_train, y_train)

# Predictions
y_pred_mlp = mlp_model.predict(X_test)

# Evaluate the model
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
print(f"MLP Neural Network Accuracy: {accuracy_mlp:.4f}")


# Stage 4: Hyperparameter Tuning with GridSearchCV
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Best model and its parameters
print("Best Random Forest Parameters:", grid_search.best_params_)

# Evaluate the best model
best_rf_model = grid_search.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)
accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)
print(f"Optimized Random Forest Accuracy: {accuracy_best_rf:.4f}")


# Results Summary
print("\n--- Model Comparison ---")
print(f"Logistic Regression Accuracy: {accuracy_log_reg:.4f}")
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")
print(f"Neural Network Accuracy: {accuracy_mlp:.4f}")
print(f"Optimized Random Forest Accuracy: {accuracy_best_rf:.4f}")

# Conclusion
print("\nConclusion:")
print("We have seen the evolution of model performance from a simple Logistic Regression to more complex models like Random Forest and Neural Networks.")
print("Further hyperparameter tuning has helped improve the performance of the Random Forest model.")

